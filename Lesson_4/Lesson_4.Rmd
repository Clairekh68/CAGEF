---
title: "Lesson 4 - Data Cleaning/Stop Wrestling with Regular Expressions"
output: 
  html_document:
          keep_md: yes
          toc: TRUE
          toc_depth: 3
  html_notebook:
          toc: TRUE
          toc_depth: 3
---
***
![](img/big-data-borat.png){width=400px} 

</br>

##A quick intro to the intro to R Lesson Series

</br>

This 'Intro to R Lesson Series' is brought to you by the Centre for the Analysis of Genome Evolution & Function's (CAGEF) bioinformatics training initiative. This course was developed based on feedback on the needs and interests of the Department of Cell & Systems Biology and the Department of Ecology and Evolutionary Biology. 



This lesson is the third in a 6-part series. The idea is that at the end of the series, you will be able to import and manipulate your data, make exploratory plots, perform some basic statistical tests, test a regression model, and make some even prettier plots and documents to share your results. 


![](img/data-science-explore.png)

</br>

How do we get there? Today we are going to be learning how to make all sorts of plots - from simple data exploration to interactive plots.The next lesson will be data cleaning and string manipulation; this is really the battleground of coding - getting your data into the format where you can analyse it. Then we will learn how to do t-tests and perform regression and modeling in R. And lastly, we will learn to write some functions, which really can save you time and help scale up your analyses.


![](img/spotify-howtobuildmvp.gif)

</br>

The structure of the class is a code-along style. It is hands on. The lecture AND code we are going through are available on GitHub for download at https://github.com/eacton/CAGEF __(Note: repo is private until approved)__, so you can spend the time coding and not taking notes. As we go along, there will be some challenge questions and multiple choice questions on Socrative. At the end of the class if you could please fill out a post-lesson survey (https://www.surveymonkey.com/r/VNQZ3KS), it will help me further develop this course and would be greatly appreciated. 

***

####Packages Used in This Lesson

The following packages are used in this lesson:

`tidyverse` (`ggplot2`, `tidyr`, `dplyr`)     
(`twitteR`)*
`tidytext`     
`viridis`     

*Used to generate the tweet tables used in this lesson. It is not necessary for you to install this - you can work from the tables. If you want to create these files - the code is here ................(insert link).    

Please install and load these packages for the lesson. In this document I will load each package separately, but I will not be reminding you to install the package. Remember: these packages may be from CRAN OR Bioconductor. 

***
####Highlighting

`grey background` - a package or function or code      
*italics* - an important term or concept     
**bold** - heading or 'grammar of graphics' term      

***
__Objective:__ At the end of this session you will be able to use regular expressions to 'clean' your data. You will also learn R markdown and be able to render your R code into slides, a pdf, html, a word document, or a notebook.

***

##Data Cleaning or Data Munging or Data Wrangling

Why do we need to do this?

'Raw' data is seldom (never) in a useable format. Data in tutorials or demos has already been meticulously filtered, transformed and readied for that specific analysis. How many people have done a tutorial only to find they can't get their own data in the @*%($! format to use the tool they have just spend an hour learning about???

Data cleaning requires us to:

- get rid of inconsistencies in our data. 
- have labels that make sense. 
- check for invalid character/numeric values.
- check for incomplete data.
- remove data we do not need.
- get our data in a proper format to be analyzed by the tools we are using. 
- flag/remove data that does not make sense.

Some definitions might take this a bit farther and include normalizing data and removing outliers, but I consider data cleaning at least, getting data into a format where we can start actively doing 'the maths or the graphs' whether it be statistical calculations, normalization or exploratory plots. We have learned how to transform data into a tidy format in Lesson 2, but the prelude to transforming data is doing the grunt work mentioned above. So let's get to it!

![](img/cleaning.gif)
</br>




##Intro to regular expressions


**Regular expressions**

"A God-awful and powerful language for expressing patterns to match in text or for search-and-replace. Frequently described as 'write only', because regular expressions are easier to write than to read/understand. And they are not particularly easy to write."
                                                                                          - Jenny Bryan

![](img/xkcd-1171-perl_problems.png)

</br>

So why does regex get so much flak?

Scary example: how to get an email in different programming languages (http://emailregex.com/). Whatever. 


![](img/80170c11996bd58e422dbb6631b73c4b.jpg) 

![](img/regexbytrialanderror-big-smaller.png)

</br>

What does the language look like?


_Matching by position_

Where is the character in the string?

```{r echo = FALSE, eval = TRUE, warning = FALSE}
library(knitr)
library(kableExtra)

text_table <- data.frame(
  Expression = c("^", "$", "\\b", "\\B"),
  Meaning = c("start of string", "end of string", "empty string at either edge of a word", "empty string that is NOT at the edge of a word")
)

kable(text_table, "html") %>%
  kable_styling(full_width = F) %>%
  column_spec(1, italic = T, border_right = T) %>%
  column_spec(2, width = "40em")
```



_Quantifiers_

How many times will a character appear?

```{r echo = FALSE, eval = TRUE, warning = FALSE}
text_table <- data.frame(
  Expression = c("?", "*","+", "{n}", "{n,}", "{,n}", "{n,m}"),
  Meaning = c("0 or 1", "0 or more", "1 or more", "exactly n", "at least n", "at most n", "between n and m (inclusive)")
)

kable(text_table, "html") %>%
  kable_styling(full_width = F) %>%
  column_spec(1, italic = T, border_right = T) %>%
  column_spec(2, width = "40em")
```


_Classes_

What kind of character is it?

```{r echo = FALSE, eval = TRUE, warning = FALSE}
text_table <- data.frame(
  Expression = c("\\w, [A-z0-9], [[:alnum:]]", "\\d, [0-9], [[:digit:]]", "[A-z], [:alpha:]", "\\s, [[:space:]]", "[[:punct:]]", "[[:lower:]]", "[[:upper:]]", "\\W, [^A-z0-9]", "\\S", "\\D, [^0-9]"),
  Meaning = c("word characters (letters + digits)", "digits", "alphabetical characters", "space", "punctuation", "lowercase", "uppercase", "not word characters", "not space", "not digits")
)

kable(text_table, "html") %>%
  kable_styling(full_width = F) %>%
  column_spec(1, italic = T, border_right = T) %>%
  column_spec(2, width = "40em")
```


_Operators_

Helper actions to match your characters.

```{r echo = FALSE, eval = TRUE, warning = FALSE}
text_table <- data.frame(
  Expression = c("|", ".", "[  ]", "[ - ]", "[^ ]", "( )"),
  Meaning = c("or", "matches any single character", "matches ANY of the characters inside the brackets", "matches a RANGE of characters inside the brackets", "matches any character EXCEPT those inside the bracket", "grouping - used for _backreferencing_")
)

kable(text_table, "html") %>%
  kable_styling(full_width = F) %>%
  column_spec(1, italic = T, border_right = T) %>%
  column_spec(2, width = "40em")
```

_Escape characters_

Sometimes a character is just a character... allows you to use a character 'as is' rather than its special function. In R, regex gets evaluated as a string before a regular expression, and a backslash is used to escape there as well - so you really need 2 backslashes to escape, say, a $ sign (`"\\\$"`). 

```{r echo = FALSE, eval = TRUE, warning = FALSE}
text_table <- data.frame(
  Expression = c("\\"),
  Meaning = c("escape! necessary to use special meta-characters (*, $, ^, ., ?, |, \\, [, ], {, }, (, )) [Note that the backslash is a meta-character as well]")
)

kable(text_table, "html") %>%
  kable_styling(full_width = F) %>%
  column_spec(1, italic = T, border_right = T) %>%
  column_spec(2, width = "40em")
```

Trouble-shooting with escaping meta-characters means adding backslashes until something works. (Joking/not joking)

![Joking/Not Joking](img/backslashes.png)

</br>

##Data Cleaning with Base R (AKA What is Elon Musk up to anyways?)

Let's take this cacaphony of characters we've just learned about and perform some basic data cleaning tasks with an actual (fun?) messy data set. I have scraped Elon Musk's latest tweets from Twitter. The code to do this is in the Lesson 4 markdown file if you are curious and/or want to creep someone on Twitter.

```{r echo = FALSE, include = FALSE, eval = FALSE }
library(twitteR)
library(httr)
#This is from the code demo for httr::oauth1-twitter
# 1. Find OAuth settings for twitter:
oauth_endpoints("twitter")

# 2. Register an application at https://apps.twitter.com/
#    Make sure to set callback url to "http://127.0.0.1:1410/"
#
#    Replace key and secret below
myapp <- oauth_app("twitter",
  key = "TYrWFPkFAkn4G5BbkWINYw",
  secret = "qjOkmKYU9kWfUFWmekJuu5tztE9aEfLbt26WlhZL8"
)

# 3. Get OAuth credentials
twitter_token <- oauth1.0_token(oauth_endpoints("twitter"), myapp)

# 4. Use API - now the code diverges from the demo
setup_twitter_oauth(consumer_key = twitter_token[["app"]][["key"]] , consumer_secret = twitter_token[["app"]][["secret"]], access_token = twitter_token[["credentials"]][["oauth_token"]], access_secret = twitter_token[["credentials"]][["oauth_token_secret"]] )

#3200 is the max number of tweets requestable
elon_tweets <- userTimeline("elonmusk", n = 3200)
elon_tweets_df <- tbl_df(map_df(elon_tweets, as.data.frame))

write.table(elon_tweets_df, "data/elon_tweets_df.txt", sep = "\t")

#used the following handles c(elon = "elonmusk", nye = "BillNye", jt = "JustinTrudeau", colbert = "StephenAtHome", jengardy = "JenniferGardy", jenny = "JennyBryan", katy = "KatyPerry", daily = "TheDailyShow", jimmy = "JimmyFallon", trump = "realDonaldTrump")
```

Let's read in the set of tweets, take a look at the structure of the data, and use 'tidyverse' to order the data by the most popular (favorited) tweets. Let's check out the top 5 favorite tweets.

```{r}
library(tidyverse)

elon_tweets_df <- read.delim("data/elon_tweets_df.txt", sep = "\t", stringsAsFactors = F)
str(elon_tweets_df)

elon_tweets_df <- elon_tweets_df %>% arrange(desc(favoriteCount))

elon_tweets_df$text[1:5]


```
Our end goal is going to be to look at the top 50 words in Elon Musk's tweets. I emphasize words, because I don't want urls, or hastags, or other tags. I also don't want punctuation or spaces. I want to extract just the words from tweets. First, I want to get remove the tags from the beginning of words. I am going to save my regex expression into an object - so we can use them again later.

What this expression says is that I want to find matches for a hastag OR an asperand followed by at least one word character. `grep` is a function that allows us to match our pattern (our expression) to a character vector.

```{r}
tags <- "#|@\\w+"

grep(pattern = tags, x = elon_tweets_df$text)

```
We can see that `grep` returns the index of the match. We have a number of entries that include tags. However, if we want to return the tweet itself instead of the index, we can use the argument `value = TRUE`. It is a good idea to do a visual inspection of your result to make sure your matches or substitutions are working the way you expected. In this case, it looks like each tweet does have a tag. We can then use `gsub` to replace that pattern (our tags) with nothing (an empty string).

```{r}

grep(tags, elon_tweets_df$text, value = TRUE) %>% head()

elon_tweets_df$text <- gsub(pattern = tags, replacement = "", elon_tweets_df$text)

```
It also looks like anything that was an apostrophe has been replaced with a string of numbers and backslashes. This is due to the fact that tweets are _encoded_ in UTF-16 and converted to UTF-8. Other things that have character codes, like emojis, will also be encoded differently. Here is an example of emoji encoding: https://raw.githubusercontent.com/today-is-a-good-day/Emoticons/master/emDict.csv. Since we are going to remove punctuation, we are going to 'convert' the encoding by substituting it with nothing (again, an empty character string). This is not something you will have to deal with on a daily basis, but character encoding is something to be aware of, especially when scraping data from the web.  

```{r}
iconv(elon_tweets_df$text, "UTF-8", "ASCII", sub="") %>% head()

elon_tweets_df$text <- iconv(elon_tweets_df$text, "UTF-8", "ASCII", sub = "")
```

Our next step would be to remove urls. This is a bit tricky. We could be looking for http:// or https:// followed by we don't know what (some combination of letters, numbers and forward slashes). 

We can check out which tweets have urls using `grep` as we did previously. We can also use `grepl` to get a logical reponse for if a tweet has a url or not. That way, if you wanted to grab all of the urls that Elon Musk suggests to visit, you can filter with `grepl`, to select all of the tweets where it is TRUE that a url is present.

However, we are going to continue our pattern of substituting what we don't want with an empty character string.

```{r}
url <- "http[s]?://[[:alnum:].\\/]+"


grep(url, elon_tweets_df$text, value = TRUE) %>% head()
grepl(url, elon_tweets_df$text) %>% head()

elon_urls <- elon_tweets_df %>% filter(grepl(url, elon_tweets_df$text))

elon_tweets_df$text <- gsub(pattern = "http[s]?://[[:alnum:].\\/]+", replacement = "", elon_tweets_df$text)

```

Lastly, we are going to get rid of trailing spaces, numbers, and punctuation all at the same time. You can find trailing at the very end of our tweet string.

```{r}
trail <- "[ ]+$|[0-9]*|[[:punct:]]"

grep(trail, elon_tweets_df$text, value = TRUE) %>% head()

elon_tweets_df$text <- gsub(pattern = trail, replacement = "", elon_tweets_df$text)

elon_tweets_df$text[1:5]
```
It looks like everything worked except that a spacing issue was created by removing all of the numbers. Let's take all of the places where there are 2 or more spaces created and substitute them with just one space. 

```{r}
space <- "\\s{2,}"

grep(space, elon_tweets_df$text, value = TRUE) %>% head()

elon_tweets_df$text <- gsub(pattern = space, replacement = " ", elon_tweets_df$text)

elon_tweets_df$text[1:5]
```
***
__Challenge__ 


<div style="float:left;margin:0 10px 10px 0" markdown="1">
![](img/maxresdefault.jpg){width=150px}

</div>

We also have a leading whitespace where we removed a number. How would we remove that whitespace?


</br>
</br>
</br>

***

```{r include = FALSE}
extra <- "^[ ]"

grep(extra, elon_tweets_df$text, value = TRUE) %>% head()

elon_tweets_df$text <- gsub(pattern = extra, replacement = "", elon_tweets_df$text)

elon_tweets_df$text[1:5]
```

Onwards!! Let's break the texts down into individual words, so we can see what the most common words used are. We can use the base R function `strsplit` to do this, in this case we want to split our tweets into words by splitting on spaces. 


```{r}
strsplit(elon_tweets_df$text, split = " ")

```
Note that the output of this function is some horrible list object. 

```{r}
str(strsplit(elon_tweets_df$text, split = " "))
```

Luckily there is an `unlist` function which recursively will go through lists to simplify their elements into a vector. Let's try it and check the structure of our output. We will save this to an object called 'words'.

```{r}
unlist(strsplit(elon_tweets_df$text, split = " "))

words <- unlist(strsplit(elon_tweets_df$text, split = " "))

str(unlist(strsplit(elon_tweets_df$text, split = " ")))

tail(words)
```
Great! But... I noticed that we missed some `\n` (newline) and `\t` (tab) characters.

***
__Challenge__ 


<div style="float:left;margin:0 10px 10px 0" markdown="1">
![](img/maxresdefault.jpg){width=150px}

</div>

Newline and tab characters are separating 2 words. Split these words apart and get rid of the newline character. Convert all of our character strings to lowercase (I haven't shown you how to do this, but I believe in your google-fu). Check the first and last 50 words to see if anything else is amiss.


</br>
</br>
</br>

***

```{r include = FALSE}
words <- tolower(unlist(strsplit(words, "\\n|\\t")))

words <- casefold(unlist(strsplit(words, "\\n|\\t")), upper = FALSE)

words[1:50]
tail(words, 50)
```

There are still a few problems with words cutoff like 'solv', or 'flamethrower' and 'flamethrowers' being the same word, or 'north' and 'korea' belonging together for context. If we were serious about this dataset we would need to resolve these issues. We also have some html and twitter-specific tags that we will deal with shortly. However, we are going to move ahead and count the number of occurences of each word and order them by frequency.

```{r}
data.frame(words) %>% count(factor(words)) %>% arrange(desc(n))
```




Wow. We have discovered people use prepositions and conjunctions, and words unrelated to content but html jargon, or things like 'na' and 'false'. There is a list of 'stop words' that can be used to get rid of words that are unlikely to contain information for us as part of the `tidytext` package. However, we will have to add to this list.

The premade dataframe is called `stop_words`. We can save it as an object and add to it by making a dataframe of the words we want to add. We can call our lexicon 'custom'.

```{r}
library(tidytext)
stop <- stop_words
str(stop)

add_stop <- data.frame(word = c("na", "false", "href", "rel", "nofollow", "true", "amp", "twitter", "iphonea", "relnofollowtwitter", "relnofollowinstagrama"), 
                       lexicon = "custom", stringsAsFactors = FALSE)
#I just should point out that true or false 'could' be a word rather than a logical, I could filter out FALSE and TRUE before changing everything to lowercase and this would reduce the likelyhood of missing 'words'
#also note that 'custom' recycles as a character vector of length 1

stop <- bind_rows(stop, add_stop)
```

To remove these stop words from our list, we perform an anti-join (from Lesson 3).

```{r}
words <- anti_join(data.frame(words), stop, by=c("words" = "word"))

```
```{r}
words %>% count(words) %>% arrange(desc(n))

words <- words %>% count(words) %>% arrange(desc(n))
```

'boring', 'falcon', 'tesla', 'rocket', 'launch','flamethrower', 'cars', 'spacex', 'tunnels', and 'mars' and 'ai' are a bit further down. There are a few words that look like they should be added to the 'stop words' list (dont, doesnt, didnt, im), but we'll work with this for now.

We can make a word cloud out of the top 50 words, which will be sized according to their frequency. I am starting with the first word after Elon Musk's twitter handle. The default color is black, but we can use our viridis package (Lesson 3) to have a pleasing color palette.

```{r}
library("wordcloud")
library("viridis")

words[2:51,] %>%
    with(wordcloud(words, n, ordered.colors = TRUE, colors = viridis(50), use.r.layout = TRUE))
```



##Data Cleaning with stringr/stringi (AKA What is Trump up to anyways?)

We are going to do the exact same data cleaning with the `stringr` package using Trump's tweets. The syntax is a little different, but it is pretty intuitive once you get started. All `stringr` functions can be found using `str_` + `Tab`. Again, we will start by loading the dataset and looking at the top 5 favorite tweets.

```{r}
trump_tweets_df <- read.delim("data/trump_tweets_df.txt", sep = "\t")


trump_tweets_df <- trump_tweets_df %>% arrange(desc(favoriteCount))
trump_tweets_df$text[1:5]

iconv(trump_tweets_df$text, "UTF-8", "ASCII", sub="") %>% head()

trump_tweets_df$text <- iconv(trump_tweets_df$text, "UTF-8", "ASCII", sub = "")
```

The first thing that we did was look for tags. The arguments are switched in `stringr` relative to the base functions. The first argument will be the character string we are searching, and the second argument will be the pattern we are matching. `str_extract` will return the index of the match, as well as the match. This is similar to `grep` when `value = TRUE`.

```{r}
str_extract(string = trump_tweets_df$text, pattern = tags)

```

`str_detect` is similar to `grepl` returning TRUE or FALSE if a match is or isn't found, respectively.

```{r}
str_detect(trump_tweets_df$text, tags)
```

Let's be ambitious and try to remove tags, urls, newline and tab characters and numbers all in one go. `str_remove` automatically replaces the match with an empty character string.

```{r}
clean <- "http[s]?://[[:alnum:].\\/]+"

#clean2 <- "#|@[a-zA-Z]+"

clean2 <- "#|@\\w+"

clean3 <- "\\$?[0-9]*%?"

clean4 <- "\\.?:?,?-?!?\\(?\\)?\\??"

clean5 <- "[[:punct:]]"

str_replace_all(trump_tweets_df$text, clean5)

trump_tweets_df$text <- str_remove_all(trump_tweets_df$text, pattern = clean)
trump_tweets_df$text <- str_remove_all(trump_tweets_df$text, pattern = clean2)
trump_tweets_df$text <- str_remove_all(trump_tweets_df$text, pattern = clean3)
trump_tweets_df$text <- str_remove_all(trump_tweets_df$text, pattern = clean4)
trump_tweets_df$text <- str_remove_all(trump_tweets_df$text, pattern = clean4)


trump_tweets_df$text[1:10]

str_extract_all(trump_tweets_df$text[1:20], pattern = clean3, simplify = TRUE)



```
`stringr` has its own function for trimming whitespace, `str_trim`, which you can use to specify whether you want leading or trailing whitespace trimmed, or both.

```{r}
trump_tweets_df$text <- str_trim(trump_tweets_df$text, side = "both")

trump_tweets_df$text[1:10]
```

See how we have a couple extra spaces in the middle of some of our strings? `str_squish` will take care of that for us, leaving only a single space between words.

```{r}
trump_tweets_df$text <- str_squish(trump_tweets_df$text)

trump_tweets_df$text[1:10]
```

All that's left is to convert all characters to lowercase, and then we can see the top Trump words!

```{r}

trump_tweets_df$text <- tolower(trump_tweets_df$text)

trump_tweets_df$text[1:10]
```

To get our tweets into a word list we use the similar function to `strsplit`, `str_split`, still splitting by the spaces betweenn words. The argument `simplify = FALSE` returns a list of character vectors which we then unlist.


```{r}
words <- unlist(str_split(trump_tweets_df$text, pattern = " ", simplify = FALSE))
```

We can now do our anti_join to remove 'stop words', and tally our remaining words and order them as before.


```{r}
words <- anti_join(data.frame(words), stop, by=c("words" = "word"))

```

```{r}
words %>% count(words) %>% arrange(desc(n))

words <- words %>% count(words) %>% arrange(desc(n))
```

Hmmm... it looks like we have those html tags in a different format. It's interesting to note these little variations because no matter how much you try to automate your analysis there is always going to be something from your new dataset that didn't fit with your old dataset. This is why we need these data wrangling skills. Even though some packages may have been created to help us on our way, they can't possibly cover every case. And they all work slighly differently.

![](img/1467481_240434926124232_550310772_n.jpg)

</br>

```{r}
add_stop <- data.frame(word = c("rel=\\nofollow\\>twitter", "href=\\http//twittercom/download/iphone\\", "iphone</a>", "<a", "&amp;", "href=\\http//twittercom//download/ipad\\", "ipad</a>"), 
                       lexicon = "custom", stringsAsFactors = FALSE)


stop <- bind_rows(stop, add_stop)

```

```{r}
words <- anti_join(data.frame(words), stop, by=c("words" = "word"))

words %>% count(words) %>% arrange(desc(n))

words <- words %>% count(words) %>% arrange(desc(n))
```
'president', 'people', 'fake', 'news', 'daca', democrats', 'jobs', 'obama', 'border', 'fbi', 'collusion', 'russia', 'wall', 'mexico' and further down is 'bad', 'crooked' and 'hillary'. 

Trump's wordcloud minus his twitter handle.
```{r}
library("wordcloud")
library("viridis")

words[2:51,] %>%
    with(wordcloud(words, n, ordered.colors = TRUE, c(3,.5),colors = viridis(50), use.r.layout = TRUE))
```

***
__Challenge__ 


<div style="float:left;margin:0 10px 10px 0" markdown="1">
![](img/maxresdefault.jpg){width=150px}

</div>

Pick one of the other tweet data sets [__insert possibilities__]. Clean it. Remove all of the stop words. Make a wordcloud of the top 50 words.


</br>
</br>
</br>

***















-rounding data - we don't need it to the umpteenth decimal
-look for lowest and highest values - do these make sense? (standard deviation?)
-called a range check, spell check, regex
-document what you are doing
-no all data sets are 100% clean
-also the paste functions

-the basics running through a fun example
- do a chosen one, see what other problems come up
-rmarkdown, syntax, etc
-make a pdf of cleaning the WellcomeTrust dataset (give a brief outline of what needs to be done)


remember regex testers
https://regex101.com/
https://regexr.com/



- searching for a word/patterns, subset using character strings, collapse and expand character vectors, replacement, replacing NAs, splitting/combining at a delimiter

-regexpr, str_locate




```{r}


save <- str_extract_all(trump_tweets_df$text, "[#|@|[:alnum:]]+([^\\s][[:alnum:]]+)?", simplify = TRUE)
#gather, get rid of empty strings
test <- gather(as.data.frame(save), value = "word") %>% filter(word != "")


```

It looks like we need some more data cleaning. First, let's get rid of everything with numbers.

```{r}
trump_words %>% select(word) %>% str_remove("[0-9]+", simplify)

#removes numbers
str_remove_all(trump_words$word, "[0-9]*")
#still have punctuation before numbers
str_remove_all(trump_words$word, "[0-9].*")


```
It's looking better. We have a single hashtag. "'s" endings should be removed - could match other words, or if a contraction will be removed via stopwords list. There is also a 'u.s' where we can get rid of the period. If anyone can find out how to remove the apostrophe and not the period, let me know.

```{r}
str_remove_all(trump_words$word, "#$")


gsub("[[:punct:]]s$", "", trump_words$word)
#let's check to see what this will remove.
grep("[[:punct:]]s$", trump_words$word, value=TRUE)
#the only thing that isn't an 's is u.s, we don't want this removed and truncated to 'u', but we also don't want to just remove the period first because we want to retain that it means united states and not us. So, I actually couldn't find a regex punctuation solution to this. BONUS points if you do. Instead, we are going to REPLACE "u.s" with "usa"

trump_words$word <- str_replace(trump_words$word, "u.s", "usa")

#check
grep("[[:punct:]]s$", trump_words$word, value=TRUE)

trump_words$word <- str_remove_all(trump_words$word, "[[:punct:]]s$")
trump_words$word <- str_remove_all(trump_words$word, "#$")

#check
grep("[[:punct:]]s$", trump_words$word, value=TRUE)


#once we know we've got it right we can filter the data frame
trump_words <- trump_words %>% mutate(word = str_remove_all(trump_words$word, "[0-9].*")) %>% filter(word != "")
```








I looked for a dataset for data cleaning and found it in a blog titled "Biologists: this is why bioinformaticians hate you...". The main and common issue with this dataset is that when data entry was done there was no structured vocabulary - meaning that people could type in whatever they wanted instead of using dropdown menus with limited options, or giving an error if something is formatted incorrectly, or stipulating some rules (ie. must be all lowercase, uppercase, no numbers, spacing, etc.). I must admit I have been guilty of messing with people who have made databases without rules. For example, in giving the emergency contact there was a line to input 'Relationship', which could easily have been a dropdown menu 'parent, partner, friend, other', but instead I was allowed to write in a free text line 'lifelong kindred spirit, soulmate and doggy-daddy'. I don't think anyone here was trying to be a nuisance, this is just a consequence of poor data collection. There is a README file to go with this spreadsheet if you have questions about the data fields.  

http://www.opiniomics.org/biologists-this-is-why-bioinformaticians-hate-you/     
https://figshare.com/articles/Wellcome_Trust_APC_spend_2012_13_data_file/963054

What I want to know is: 
1. List 5 problems with this data set.
1. Which publisher is the most expensive to publish with?
1. Which journal is the most expensive to publish with? Is this by the same publisher?                  
1. Convert sterling to CAD. What is the median cost of publishing with Elsevier in CAD?

The blogger's opinion of cleaning this dataset:

'I now have no hair left; I’ve torn it all out.  My teeth are just stumps from excessive gnashing.  My faith in humanity has been destroyed!'

Don't get to this point. The dataset doesn't need to be perfect. Just do what you gotta do to answer these questions.  

Note to self: This may be too tough - see how long it takes to do.

Approximate time: 2 hours per lesson

 _Each lesson will have:_
 
 - Comprehension questions as we go along on Socrative.
 - How to read help pages online.
 - Give the class a function not previously used during the lesson and have them figure out what it does and how to use it.
 - Each lesson will start from an excel spreadsheet with imperfect data.

***

_R markdown and knitr_

- r markdown syntax
    +  making things pretty: adding table of contents, images, hyperlinks, urls
- knitr code chunk options
    + suppressing pkg load warnings, eval = T/F, re-running some chunks while keeping others in memory
    + tables in knitr
- rendering to pdf, html, word documents (any interest in slides?)
- sharing on Rpubs

__Challenge:__      

Take the original gapminder dataset and covert it to the 'clean' dataset found in the gapminder package / find some horrible dataset to clean. Present in a knitr table, explaining some of your data cleaning challenges in rmarkdown. Knit the document to a pdf.
   
__Resources:__     
http://stat545.com/block022_regular-expression.html
http://stat545.com/block027_regular-expressions.html
http://stat545.com/block028_character-data.html     
http://r4ds.had.co.nz/strings.html
http://www.gastonsanchez.com/Handling_and_Processing_Strings_in_R.pdf     
http://varianceexplained.org/r/trump-tweets/     
http://www.opiniomics.org/biologists-this-is-why-bioinformaticians-hate-you/     
https://figshare.com/articles/Wellcome_Trust_APC_spend_2012_13_data_file/963054

##Post-Lesson Assessment
***
_Questions_

- Speed: Too slow, too fast, just right
- Content: Too easy, too hard, just right
- From the description of the lesson, the content was what I expected to learn. T/F
- What was the most useful thing you learned?
- What was the least useful thing?
- Comments/suggestions for improvement.


##Notes
